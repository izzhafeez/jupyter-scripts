{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilcoxon import geo\n",
    "from wilcoxon import sheets\n",
    "from wilcoxon import spiderman\n",
    "from math import sin, cos, atan2, radians, sqrt\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import geoplot\n",
    "from abc import ABC, abstractmethod\n",
    "import pyarrow.parquet as pq\n",
    "from shapely.ops import transform\n",
    "from functools import partial\n",
    "import shapely\n",
    "import re\n",
    "import fiona\n",
    "import json\n",
    "import requests\n",
    "import operator\n",
    "import os\n",
    "import datetime as dt\n",
    "import time\n",
    "import pyproj\n",
    "\n",
    "class DistanceCalculator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_distance_between(point1, point2):\n",
    "        R = 6373\n",
    "        lat1 = radians(point1.y)\n",
    "        long1 = radians(point1.x)\n",
    "        lat2 = radians(point2.y)\n",
    "        long2 = radians(point2.x)\n",
    "        dlat = lat2 - lat1\n",
    "        dlong = long2 - long1\n",
    "        a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlong / 2)**2\n",
    "        return round(R*2*atan2(sqrt(a), sqrt(1-a)), 3)\n",
    "    \n",
    "def get_planning():\n",
    "    PATH_TO_PLANNING_AREAS = \"../Geospatial/2022/subzone-census-2010/Subzone_Census2010.kml\"\n",
    "    gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "    planning = gpd.read_file(PATH_TO_PLANNING_AREAS, driver='KML')\n",
    "    planning[\"SubzoneCode\"] = planning.Description.str.extract(\"Subzone Code.*?<td>(.*?)</td>\")\n",
    "    planning[\"Planning\"] = planning.Description.str.extract(\"Planning Area Name.*?<td>(.*?)</td>\")\n",
    "    planning[\"PlanningCode\"] = planning.Description.str.extract(\"Planning Area Code.*?<td>(.*?)</td>\")\n",
    "    planning[\"Region\"] = planning.Description.str.extract(\"Region Name.*?<td>(.*?)</td>\")\n",
    "    planning[\"RegionCode\"] = planning.Description.str.extract(\"Region Code.*?<td>(.*?)</td>\")\n",
    "    planning = planning.rename(columns={\"Name\":\"Subzone\"})\n",
    "    planning = planning.drop(\"Description\",axis=1)\n",
    "    planning = planning[['Region', 'RegionCode', 'Planning', 'PlanningCode', 'Subzone', 'SubzoneCode', 'geometry']]\n",
    "    return planning\n",
    "\n",
    "file_names = os.listdir(\"../Geospatial/2022/city=Singapore\")\n",
    "grab = pd.concat(\n",
    "    [pq.read_table(\"../Geospatial/2022/city=Singapore/\"+file_name).to_pandas()\n",
    "     for file_name in file_names])\n",
    "grab[\"rawlng\"] = grab.rawlng.round(5)\n",
    "grab[\"rawlat\"] = grab.rawlat.round(5)\n",
    "grab[\"speed\"] = grab.speed.round(0).astype(int)\n",
    "grab[\"accuracy\"] = grab.accuracy.round(0).astype(int)\n",
    "grab = grab.drop([\"bearing\", \"driving_mode\"], axis=1)\n",
    "grab[\"osname\"] = grab.osname.map({\"android\": 0, \"ios\": 1})\n",
    "grab[\"trj_id\"] = grab.trj_id.astype(int)\n",
    "grab = grab.sort_values(by=[\"trj_id\",\"pingtimestamp\"])\n",
    "# grab = grab[(grab.speed >= 0) & (grab.accuracy < 100)]\n",
    "grab = gpd.GeoDataFrame(grab, geometry=gpd.points_from_xy(grab.rawlng, grab.rawlat))\n",
    "grab = grab.drop([\"accuracy\", \"rawlat\", \"rawlng\"], axis=1)\n",
    "grab.columns = [\"id\", \"os\", \"time\", \"speed\", \"geometry\"]\n",
    "grab = grab.set_index(\"id\")\n",
    "\n",
    "print(\"ok\")\n",
    "grab_line = grab.groupby(\"id\").agg({\n",
    "    \"os\": max,\n",
    "    \"time\": (max, min),\n",
    "    \"geometry\": (lambda x: shapely.geometry.LineString(x.tolist()), lambda x: x.iloc[0], lambda x: x.iloc[-1])})\n",
    "\n",
    "grab_line[\"distance\"] = grab_line.geometry[\"<lambda_0>\"].apply(lambda x: x.length*111.33)\n",
    "grab_line[\"os\"] = grab_line.os[\"max\"]\n",
    "grab_line[\"start_time\"] = grab_line.time[\"min\"]\n",
    "grab_line[\"end_time\"] = grab_line.time[\"max\"]\n",
    "grab_line[\"geometry1\"] = grab_line.geometry[\"<lambda_0>\"]\n",
    "grab_line[\"start_point\"] = grab_line.geometry[\"<lambda_1>\"]\n",
    "grab_line[\"end_point\"] = grab_line.geometry[\"<lambda_2>\"]\n",
    "grab_line = grab_line[[\"os\", \"start_time\", \"start_point\", \"end_time\", \"end_point\", \"geometry1\", \"distance\"]]\n",
    "grab_line_gdf = gpd.GeoDataFrame(grab_line, geometry=\"geometry1\")\n",
    "# grab_line_gdf[\"start_point\"] = grab_line_gdf.start_point.apply(lambda x: x.coords[0])\n",
    "# grab_line_gdf[\"end_point\"] = grab_line_gdf.end_point.apply(lambda x: x.coords[-1])\n",
    "\n",
    "def find_max_dist(line):\n",
    "    max_dist = 0\n",
    "    c_coord = shapely.geometry.Point(line.coords[0])\n",
    "    for coord in line.coords:\n",
    "        p_coord = shapely.geometry.Point(coord)\n",
    "        max_dist = max(max_dist, DistanceCalculator.get_distance_between(p_coord, c_coord))\n",
    "        c_coord = p_coord\n",
    "    return max_dist\n",
    "\n",
    "max_dists = []\n",
    "for line in tqdm(lines):\n",
    "    max_dists.append(find_max_dist(line))\n",
    "\n",
    "grab_line_gdf[\"max_dist\"] = max_dists\n",
    "grab = grab[grab_line_gdf[\"max_dist\"]<0.1]\n",
    "grab_line_gdf = grab_line_gdf[grab_line_gdf[\"max_dist\"] < 0.1]\n",
    "grab_subset = grab.iloc[::10]\n",
    "\n",
    "planning = get_planning()\n",
    "planningm = planning[[\"PlanningCode\", \"SubzoneCode\", \"geometry\"]].values.tolist()\n",
    "planning_dict = planning.set_index(\"SubzoneCode\").to_dict(\"index\")\n",
    "points = grab_subset.geometry.tolist()\n",
    "\n",
    "points_in_planning = []\n",
    "for point in tqdm(points):\n",
    "    not_done = True\n",
    "    for p_code, s_code, geom in planningm:\n",
    "        if point.within(geom):\n",
    "            points_in_planning.append(s_code)\n",
    "            not_done = False\n",
    "            break\n",
    "    if not_done:\n",
    "        points_in_planning.append(np.nan)\n",
    "        print(\"not ok\")\n",
    "grab_subset[\"plan\"] = points_in_planning\n",
    "grab_subset = grab_subset[~pd.isna(grab_subset.plan)]\n",
    "grab_subset[\"time\"] = grab_subset.time.apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "grab_subset[\"day\"] = grab_subset.time.apply(lambda x: x.day)\n",
    "grab_subset[\"w_day\"] = grab_subset.time.apply(lambda x: x.weekday())\n",
    "grab_subset[\"hour\"] = grab_subset.time.apply(lambda x: x.hour)\n",
    "grab_subset[\"time\"] = grab_subset.time.apply(str)\n",
    "grab_line_gdf[\"day\"] = grab_line_gdf[\"start_time\"].apply(lambda x: dt.datetime.fromtimestamp(x).day)\n",
    "grab_line_gdf[\"hour\"] = grab_line_gdf[\"start_time\"].apply(lambda x: dt.datetime.fromtimestamp(x).hour)\n",
    "grab_line_gdf[\"weekday\"] = grab_line_gdf[\"start_time\"].apply(lambda x: dt.datetime.fromtimestamp(x).weekday())\n",
    "grab_line_gdf[\"duration\"] = (grab_line_gdf.end_time.apply(dt.datetime.fromtimestamp) - grab_line_gdf.start_time.apply(dt.datetime.fromtimestamp)).apply(lambda x: round(x.seconds/60, 2))\n",
    "grab_line_gdf[\"speed\"] = grab_line_gdf[\"distance\"] / grab_line_gdf.duration\n",
    "grab_line_gdf[\"s_distance\"] = grab_line_gdf[[\"start_point\",\"end_point\"]].apply(lambda x: DistanceCalculator.get_distance_between(x.start_point, x.end_point), axis=1)\n",
    "grab_line_gdf[\"efficiency\"] = grab_line_gdf.s_distance / grab_line_gdf[\"distance\"]\n",
    "grab_line_gdf[\"t_efficiency\"] = grab_line_gdf.s_distance / grab_line_gdf.duration\n",
    "grab_line_gdf.drop([\"start_point\",\"end_point\"], axis=1).to_file(\"../Geospatial/2022/grab-line-computed.geojson\")\n",
    "\n",
    "def getMRT():\n",
    "    \"\"\"\n",
    "    PURPOSE (NO LONGER WORKS FULLY)\n",
    "\n",
    "    I made this code to extract all the kinds of data you'd want to know about MRT and LRT stations.\n",
    "    More specific information will be given in comments below but as of now, here is the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vQIXKejfbHe2FuPKp0qvdS6OfLDceHGO2WvxGgJtyuoD6HyAmd9Qxj8NZsJA4JItvPcqRKyMJUdDVF-/pub?gid=1134364680&single=true&output=pdf\n",
    "    Do note that this is not the full version of the dataset, as I wanted to only keep the essentials in there.\n",
    "\n",
    "    Download data for train station exits: TrainStationExit_Jan2020/TrainStationExit06032020.shp\n",
    "    Download data for train stations: TrainStation_Jan2020/MRTLRTStnPtt.shp\n",
    "    Download data for OriginDestinationTrain: origin_destination_train_202103.csv\n",
    "    \"\"\"\n",
    "    # path_to_TrainStationExits = \"\"\n",
    "    # path_to_TrainStations = \"\"\n",
    "    # path_to_OriginDestinationTrain = \"\"\n",
    "\n",
    "\n",
    "    print(\"Extracting Wikipedia links of MRT and LRT stations.\")\n",
    "    \"\"\"\n",
    "    PURPOSE\n",
    "\n",
    "    DOES NOT WORK NOW BECAUSE WIKIPEDIA CHANGED THE FORMAT OF THE PAGE.\n",
    "    But what this section does is extract the links to all the Wikipedia pages of the respective MRT and LRT stations.\n",
    "    Changi Airport, Expo, Tanah Merah and Damai LRT were added manually because they were not found anywhere within the landing page.\n",
    "    \"\"\"\n",
    "    stations = website(\"https://en.wikipedia.org/wiki/List_of_Singapore_MRT_stations\")\n",
    "    stations.getTables()\n",
    "    stationLinksM = stations.tables[1][\"Links\"].apply(lambda x: x.split(\"\\n\")[0]).tolist()\n",
    "    stationLinksM = [x for x in stationLinksM if (\"a\"+x)[-1] == \"n\"]\n",
    "    stationLinksM.extend([\"https://en.wikipedia.org/wiki/Changi_Airport_MRT_station\",\n",
    "                         \"https://en.wikipedia.org/wiki/Expo_MRT_station\",\n",
    "                         \"https://en.wikipedia.org/wiki/Tanah_Merah_MRT_station\"])\n",
    "\n",
    "    stations = website(\"https://en.wikipedia.org/wiki/List_of_Singapore_LRT_stations\")\n",
    "    stations.getTables()\n",
    "    stationLinksL = stations.tables[0][\"Links\"].apply(lambda x: x.split(\"\\n\")[0]).tolist()\n",
    "    stationLinksL = [x for x in stationLinksL if (\"a\"+x)[-1] == \"n\"]\n",
    "    stationLinksL.extend([\"https://en.wikipedia.org/wiki/Damai_LRT_station_(Singapore)\"])\n",
    "\n",
    "    stationLinks = list(set(stationLinksM + stationLinksL))\n",
    "\n",
    "\n",
    "    print(\"Extracting data from Wikipedia pages.\")\n",
    "    \"\"\"\n",
    "    PURPOSE\n",
    "\n",
    "    Each page within the links has many key information scattered in various places.\n",
    "    It was a challenge to make sure all the information from every page was captured, even if the data is inconsistently placed.\n",
    "    The whole section below is to extract:\n",
    "        Station Labels (NS26, EW14)\n",
    "        Station Name (Raffles Place)\n",
    "        Chinese Translation (莱佛士坊)\n",
    "        Address (5 Raffles Place)\n",
    "        Postal Code (048618)\n",
    "        Latitude (Derived from 1°17′1.97″)\n",
    "        Longitude (Derived from 103°51′5.52″)\n",
    "    \"\"\"\n",
    "    mrt = []\n",
    "\n",
    "    for link in tqdm(stationLinks):\n",
    "        site = website(link)\n",
    "        try:\n",
    "            stationName = site.html(\"title\")[0].string.split(\" MRT\")[0]\n",
    "        except Exception as e:\n",
    "            print(\"stationName gave the error: \" + str(e) + \"\\n\" + link)\n",
    "            stationName = None\n",
    "\n",
    "        try:\n",
    "            stationDetails = list(site.html.find(class_=\"fn org\").stripped_strings)\n",
    "            chineseIndex = 0\n",
    "            for detail in stationDetails:\n",
    "                if re.compile('[\\u4e00-\\u9fff]+').search(detail):\n",
    "                    chineseIndex = stationDetails.index(detail)\n",
    "            stationLabels = \", \".join(stationDetails[:chineseIndex-1])\n",
    "            stationChinese = stationDetails[chineseIndex]\n",
    "        except Exception as e:\n",
    "            print(\"stationLabels gave the error: \" + str(e) + \"\\n\" + link)\n",
    "            stationLabels = None\n",
    "            stationChinese = None\n",
    "\n",
    "        try:\n",
    "            lat = convertCoords(*re.findall(\"[0-9.]+\", site.html(class_=\"latitude\")[0].text))\n",
    "            long = convertCoords(*re.findall(\"[0-9.]+\", site.html(class_=\"longitude\")[0].text))\n",
    "        except Exception as e:\n",
    "            print(\"latlong gave the error: \" + str(e) + \"\\n\" + link)\n",
    "            lat = None\n",
    "            long = None\n",
    "\n",
    "        try:\n",
    "            fullAddress = list(site.html(class_=\"infobox-data\")[0].strings)\n",
    "            address = fullAddress[0]\n",
    "            postcode = re.findall(r\"\\d{6}\", fullAddress[1])[0]\n",
    "        except Exception as e:\n",
    "            print(\"address gave the error: \" + str(e) + \"\\n\" + link)\n",
    "            address = None\n",
    "            postcode = None\n",
    "        mrt.append({\"Label\": stationLabels, \"Name\": stationName, \"Chinese\": stationChinese, \"Address\": address, \"Postcode\": postcode, \"Lat\": lat, \"Long\": long, \"Link\": link})\n",
    "\n",
    "grab.groupby(\"id\").last().to_file(\"../Geospatial/Sheets/grab-end.geojson\")\n",
    "# grab_line_gdf.drop([\"start_point\",\"end_point\"], axis=1).to_file(\"../Geospatial/Sheets/grab-line-computed.geojson\")\n",
    "grab_line_gdf.drop([\"start_point\",\"end_point\"], axis=1).to_file(\"../Geospatial/Sheets/grab-line-computed.geojson\")\n",
    "# mp = grab_line_gdf.loc[list(grab_subset[grab_subset.plan.str.contains(\"MP\")].index.unique())]\n",
    "cg = grab_subset.loc[list(grab_subset[grab_subset.plan.str.contains(\"CHS\")].index.unique())]\n",
    "mrt_gdf = gpd.GeoDataFrame(mrt, geometry=gpd.points_from_xy(mrt.Long, mrt.Lat))\n",
    "mrt_gdf.columns = [\"label\",\"abbr\",\"name\",\"chinese\",\"o_year\",\"c_year\",\"address\",\"postcode\",\"lat\",\"long\",\"geometry\"]\n",
    "mrt_gdf.to_file(\"../Geospatial/Sheets/mrt.geojson\")\n",
    "planning = get_planning()\n",
    "planning.columns = [\"region\",\"r_code\",\"planning\",\"p_code\",\"subzone\",\"s_code\",\"geometry\"]\n",
    "planning_avg_speed = grab_subset.groupby(\"plan\").speed.agg(np.mean).reset_index()\n",
    "planning_grab = planning.merge(planning_avg_speed, left_on=\"s_code\", right_on=\"plan\")\n",
    "gpd.GeoDataFrame(planning_grab, geometry=\"geometry\").to_file(\"../Geospatial/Sheets/planning-grab.geojson\")\n",
    "# grab_subset[\"time\"] = grab_subset.time.apply(lambda x: dt.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "grab_subset[\"time\"] = grab_subset.time.apply(lambda x: x.timestamp())\n",
    "planning_avg_speed = grab_subset.groupby(\"plan\").speed.agg(np.mean).reset_index()\n",
    "grab_subset.to_file(\"../Geospatial/Sheets/grab-subset.geojson\")\n",
    "malls = malls[malls.Latitude > 0]\n",
    "gpd.GeoDataFrame(malls, geometry=gpd.points_from_xy(malls.Longitude, malls.Latitude)).to_file(\"../Geospatial/Sheets/malls.geojson\")\n",
    "grab_subset.groupby(\"id\").last().merge(grab_line_gdf[[\"duration\", \"s_distance\", \"t_efficiency\"]], left_index=True, right_index=True, how=\"left\").to_file(\"../Geospatial/Sheets/grab-end.geojson\")\n",
    "planning = get_planning()\n",
    "planningm = planning[[\"PlanningCode\", \"SubzoneCode\", \"geometry\"]].values.tolist()\n",
    "planning_dict = planning.set_index(\"SubzoneCode\").to_dict(\"index\")\n",
    "s_points = grab_line_gdf.start_point.tolist()\n",
    "e_points = grab_line_gdf.end_point.tolist()\n",
    "\n",
    "s_points_in_planning = []\n",
    "for point in tqdm(s_points):\n",
    "    not_done = True\n",
    "    for p_code, s_code, geom in planningm:\n",
    "        if point.within(geom):\n",
    "            s_points_in_planning.append(s_code)\n",
    "            not_done = False\n",
    "            break\n",
    "    if not_done:\n",
    "        s_points_in_planning.append(np.nan)\n",
    "        print(\"not ok\")\n",
    "        \n",
    "e_points_in_planning = []\n",
    "for point in tqdm(e_points):\n",
    "    not_done = True\n",
    "    for p_code, s_code, geom in planningm:\n",
    "        if point.within(geom):\n",
    "            e_points_in_planning.append(s_code)\n",
    "            not_done = False\n",
    "            break\n",
    "    if not_done:\n",
    "        e_points_in_planning.append(np.nan)\n",
    "        print(\"not ok\")\n",
    "grab_line_gdf[\"start_plan\"] = s_points_in_planning\n",
    "grab_line_gdf[\"end_plan\"] = e_points_in_planning\n",
    "grab_line_plan = grab_line_gdf.groupby(\"start_plan\").speed.agg(np.mean)\n",
    "grab_line_plan = grab_line_plan.sort_values().reset_index()\n",
    "grab_line_plan.columns = [\"Planning Subzone\", \"Speed of journey\"]\n",
    "grab_line_plan[\"Planning Subzone\"] = grab_line_plan[\"Planning Subzone\"].apply(lambda x: planning_dict[x][\"Planning\"])\n",
    "grab_line_plan[\"Speed of journey\"] = grab_line_plan[\"Speed of journey\"] / 60 * 1000\n",
    "grab_line_gdf.drop([\"start_point\",\"end_point\"],axis=1).to_file(\"../Geospatial/Sheets/grab-line-computed.geojson\")\n",
    "grab_line_gdf[grab_line_gdf.start_plan.str.contains(\"HGS\")].end_plan.apply(lambda x: planning_dict[x][\"Planning\"]).value_counts()\n",
    "\n",
    "# %store grab\n",
    "# %store grab_line\n",
    "# %store -r grab\n",
    "# grab_copy = grab.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
